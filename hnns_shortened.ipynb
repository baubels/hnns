{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "55b3ae9b-f96d-4d5d-83db-63d204c1024d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-28T22:25:54.975488Z",
          "iopub.status.busy": "2022-10-28T22:25:54.975156Z",
          "iopub.status.idle": "2022-10-28T22:25:54.980935Z",
          "shell.execute_reply": "2022-10-28T22:25:54.979204Z",
          "shell.execute_reply.started": "2022-10-28T22:25:54.975456Z"
        },
        "id": "55b3ae9b-f96d-4d5d-83db-63d204c1024d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "98863019-0c34-4b36-937f-e5630f253ee2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-28T22:25:55.290529Z",
          "iopub.status.busy": "2022-10-28T22:25:55.289935Z",
          "iopub.status.idle": "2022-10-28T22:25:55.299614Z",
          "shell.execute_reply": "2022-10-28T22:25:55.297651Z",
          "shell.execute_reply.started": "2022-10-28T22:25:55.290465Z"
        },
        "id": "98863019-0c34-4b36-937f-e5630f253ee2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Define relevant variables for the ML task\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "learning_rate = 1e-7\n",
        "num_epochs = 2\n",
        "\n",
        "# Device will determine whether to run the training on GPU or CPU.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "acad9690-e92d-4fa8-b22d-ba9460cbce44",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-28T22:25:55.507600Z",
          "iopub.status.busy": "2022-10-28T22:25:55.507233Z",
          "iopub.status.idle": "2022-10-28T22:25:55.560562Z",
          "shell.execute_reply": "2022-10-28T22:25:55.559976Z",
          "shell.execute_reply.started": "2022-10-28T22:25:55.507566Z"
        },
        "id": "acad9690-e92d-4fa8-b22d-ba9460cbce44",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Use transforms.compose method to reformat images for modeling,\n",
        "# and save to variable all_transforms for later use\n",
        "all_transforms = transforms.Compose([transforms.Resize((32,32)),\n",
        "                                     transforms.ToTensor()])\n",
        "\n",
        "# Create Training dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root = './data',\n",
        "                                             train = True,\n",
        "                                             transform = all_transforms,\n",
        "                                             download = True)\n",
        "\n",
        "# Create Testing dataset\n",
        "test_dataset = torchvision.datasets.MNIST(root = './data',\n",
        "                                            train = False,\n",
        "                                            transform = all_transforms,\n",
        "                                            download=True)\n",
        "\n",
        "# Instantiate loader objects to facilitate processing\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "21191649-42a3-499d-9e1f-b4ed250120cc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-28T22:25:55.659338Z",
          "iopub.status.busy": "2022-10-28T22:25:55.659030Z",
          "iopub.status.idle": "2022-10-28T22:25:55.676711Z",
          "shell.execute_reply": "2022-10-28T22:25:55.675177Z",
          "shell.execute_reply.started": "2022-10-28T22:25:55.659307Z"
        },
        "id": "21191649-42a3-499d-9e1f-b4ed250120cc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class My2DConvLayer(nn.Module):\n",
        "    \"\"\" Locally rotationally equivariant-baked CNNs (Harmonic Net implementation).\"\"\"\n",
        "    def __init__(self, kernel_size, rotation_speed, in_channels, out_channels):\n",
        "        # Construct a convolution of size kernel_size[0] (odd int) x kernel_size[1] (odd int)\n",
        "        # specified by spherical harmonics with specified `rotation_speed`.\n",
        "        # in_channels: int  - the last dimension of the input signal\n",
        "        # out_channels: int - the specified last dimension of the output signal\n",
        "        super().__init__()\n",
        "        assert (type(kernel_size[0]==int) and type(kernel_size[1]) == int \n",
        "                and kernel_size[0]%2 == 1 and kernel_size[0]%2 == 1), 'Ensure kernel is odd-sized and of type tuple(int, int)'\n",
        "        self.kernel_width, self.kernel_depth = kernel_size[1], kernel_size[0]\n",
        "        self.in_channels, self.out_channels = in_channels, out_channels\n",
        "        \n",
        "        # Construct filters via spherical harmonics: R(r)*exp(i(m*phi + beta))\n",
        "        # R(r): a function of the radial distance r\n",
        "        # m: the rotation speed parameters\n",
        "        # beta: a learnable rotation start parameter\n",
        "        # R(r) is made a polynomial, here of order 2\n",
        "        self.weights = nn.Parameter(torch.Tensor(2+2, 1, 1, self.out_channels)).to(device)\n",
        "        torch.nn.init.uniform_(self.weights, -3.14/2, 3.14/2)\n",
        "        self.weights = self.weights.to(device)\n",
        "        \n",
        "        # 2d kernel generation\n",
        "        # construct harmonic kernels tensor\n",
        "        r, theta = self._generate_meshgrid()\n",
        "\n",
        "        self.kernels = ((self.weights[0]*r**0 + self.weights[1]*r + \n",
        "                         self.weights[2]*r**2)*\n",
        "                         torch.exp((rotation_speed*theta + self.weights[3])*1j))\n",
        "\n",
        "        # reshape for better use with forward pass\n",
        "        self.kernels = torch.transpose(self.kernels, 2, 0)           # shape: out_channels, kernel_size (unpacked)\n",
        "        self.kernels = self.kernels[None,:,None,:,:,None].to(device) # shape: 1, out_channels, 1, kernel_size (unpacked), 1\n",
        "\n",
        "\n",
        "    def _generate_meshgrid(self):\n",
        "        # generate coordinate space meshgrid with centre at (0,0)\n",
        "        y, x = torch.meshgrid(torch.arange((-self.kernel_width+1)//2, (self.kernel_width+1)//2), \n",
        "                              torch.arange((-self.kernel_depth+1)//2, (self.kernel_depth+1)//2))\n",
        "        x, y = x, -y\n",
        "        # convert cartesian grid to that of polar, fixing non-surjective conversions manually\n",
        "        r     = torch.sqrt(x**2 + y**2)\n",
        "        theta = torch.atan(y/x)\n",
        "        theta[:self.kernel_width//2,:self.kernel_depth//2] += torch.pi # fix 2nd quadrant\n",
        "        theta[self.kernel_width//2:,:self.kernel_depth//2] -= torch.pi # fix 3rd quadrant\n",
        "        theta = torch.nan_to_num(theta)\n",
        "        \n",
        "        # add a dimension for help with broadcasting later\n",
        "        r, theta = r[:,:,None].to(device), theta[:,:,None].to(device)\n",
        "        return r, theta\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x is of shape (batch_size, 3, 32, 32)\n",
        "\n",
        "        # perform a convolution, no padding\n",
        "        # assuming the input is, without batching and empty dimensions, a rank 3 tensor\n",
        "        conv_output = torch.zeros(x.shape[0], # <- batch size\n",
        "                                  x.shape[2]-self.kernel_depth+1, \n",
        "                                  x.shape[3]-self.kernel_width+1, \n",
        "                                  self.out_channels, \n",
        "                                  dtype=torch.cdouble).to(device)\n",
        "        \n",
        "        # expanding dims for broadcasting reasons\n",
        "        # size of batch_size, 1, 1, 3, 32, 32\n",
        "        xcopy = torch.clone(x)\n",
        "        xcopy = xcopy[:, None, None, :, :, :]\n",
        "        xcopy = torch.transpose(xcopy, 5, 3)\n",
        "        for w in range(x.shape[2]-self.kernel_depth):\n",
        "            for h in range(x.shape[3]-self.kernel_width):\n",
        "                conv_output[:,h,w] = torch.sum(self.kernels*\n",
        "                                                xcopy[:,:,:,h:h+self.kernel_width,w:w+self.kernel_depth,:], \n",
        "                                                dim=[2, 3, 4, 5])\n",
        "        \n",
        "        return torch.transpose(conv_output, 3, 1)\n",
        "\n",
        "\n",
        "class ComplexRelu(nn.Module):\n",
        "    # Custom Complex Relu function defined by re^(i * theta) |-> |r|e^(i * theta).\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.nn.functional.relu(abs(x))*x\n",
        "\n",
        "\n",
        "class ComplexMeanPooling(nn.Module):\n",
        "    \"\"\" Complex valued mean pooling.\"\"\"\n",
        "    def __init__(self, kernel_size, stride):\n",
        "        super().__init__()\n",
        "        assert (type(kernel_size[0]==int) and type(kernel_size[1]) == int and type(stride) == int), 'Ensure kernel is of type tuple(int, int) and stride is of type int.'\n",
        "        self.kernel_width, self.kernel_depth = kernel_size[1], kernel_size[0]\n",
        "        self.stride = stride\n",
        "        self.kernels = torch.ones((1, 1, self.kernel_width, self.kernel_depth)).to(device)\n",
        "        self.kernels *= 1./(self.kernel_width*self.kernel_depth)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x is of shape [batch_size, prior out_channels, size, size]\n",
        "        depth_shape = (x.shape[2] + 1 - self.kernel_depth)//self.stride\n",
        "        width_shape = (x.shape[3] + 1 - self.kernel_width)//self.stride\n",
        "\n",
        "        conv_output = torch.zeros((x.shape[0],x.shape[1],\n",
        "                                   depth_shape,\n",
        "                                   width_shape),\n",
        "                                   dtype=torch.cdouble).to(device)\n",
        "        \n",
        "        for w in range(width_shape):\n",
        "            for h in range(depth_shape):\n",
        "                conv_output[:,:,h,w] = torch.sum(self.kernels*\n",
        "                                                 x[:,:,self.stride*h:self.stride*h+self.kernel_depth,self.stride*w:self.stride*w+self.kernel_width], \n",
        "                                                 dim=[2,3])\n",
        "        return conv_output\n",
        "\n",
        "\n",
        "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
        "    # Use `is_grad_enabled` to determine whether we are in training mode\n",
        "    if not torch.is_grad_enabled():\n",
        "        # In prediction mode, use mean and variance obtained by moving average\n",
        "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
        "    else:\n",
        "        assert len(X.shape) in (2, 4)\n",
        "        if len(X.shape) == 2:\n",
        "            # When using a fully connected layer, calculate the mean and\n",
        "            # variance on the feature dimension\n",
        "            mean = X.mean(dim=0)\n",
        "            var = ((X - mean) ** 2).mean(dim=0)\n",
        "        else:\n",
        "            # When using a two-dimensional convolutional layer, calculate the\n",
        "            # mean and variance on the channel dimension (axis=1). Here we\n",
        "            # need to maintain the shape of `X`, so that the broadcasting\n",
        "            # operation can be carried out later\n",
        "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
        "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
        "        # In training mode, the current mean and variance are used\n",
        "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
        "        # Update the mean and variance using moving average\n",
        "        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean\n",
        "        moving_var = (1.0 - momentum) * moving_var + momentum * var\n",
        "    Y = gamma * X_hat + beta  # Scale and shift\n",
        "    return Y, moving_mean.data, moving_var.data\n",
        "\n",
        "\n",
        "\n",
        "class BatchNorm(nn.Module):\n",
        "    # `num_features`: the number of outputs for a fully connected layer\n",
        "    # or the number of output channels for a convolutional layer. `num_dims`:\n",
        "    # 2 for a fully connected layer and 4 for a convolutional layer\n",
        "    def __init__(self, num_features, num_dims):\n",
        "        super().__init__()\n",
        "        if num_dims == 2: shape = (1, num_features)\n",
        "        else: shape = (1, num_features, 1, 1)\n",
        "        # The scale parameter and the shift parameter (model parameters) are\n",
        "        # initialized to 1 and 0, respectively\n",
        "        self.gamma = nn.Parameter(torch.ones(shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(shape))\n",
        "        # The variables that are not model parameters are initialized to 0 and 1\n",
        "        self.moving_mean = torch.zeros(shape, dtype=torch.cdouble)\n",
        "        self.moving_var = torch.ones(shape, dtype=torch.cdouble)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # If `X` is not on the main memory, copy `moving_mean` and\n",
        "        # `moving_var` to the device where `X` is located\n",
        "        if self.moving_mean.device != X.device:\n",
        "            self.moving_mean = self.moving_mean.to(X.device)\n",
        "            self.moving_var = self.moving_var.to(X.device)\n",
        "        # Save the updated `moving_mean` and `moving_var`\n",
        "\n",
        "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
        "            X, self.gamma, self.beta, self.moving_mean,\n",
        "            self.moving_var, eps=1e-5, momentum=0.1)\n",
        "        return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "9ffe8587-04d6-43a1-9b83-2fac9408e861",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-28T22:25:56.055274Z",
          "iopub.status.busy": "2022-10-28T22:25:56.054442Z",
          "iopub.status.idle": "2022-10-28T22:25:56.066626Z",
          "shell.execute_reply": "2022-10-28T22:25:56.065471Z",
          "shell.execute_reply.started": "2022-10-28T22:25:56.055230Z"
        },
        "id": "9ffe8587-04d6-43a1-9b83-2fac9408e861",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Make Harmonic Neural Net\n",
        "class HarmonicNeuralNet(nn.Module):\n",
        "\t#  Determine what layers and their order in CNN object\n",
        "    def __init__(self, num_classes):\n",
        "        super(HarmonicNeuralNet, self).__init__()\n",
        "        self.conv_layer1 = My2DConvLayer(kernel_size=(5, 5), rotation_speed=1, in_channels=3, out_channels=16)\n",
        "        self.celu1 = ComplexRelu()\n",
        "        self.bn1 = BatchNorm(16, 4)\n",
        "        self.avgpool1 = ComplexMeanPooling(kernel_size=(5,5), stride=2)\n",
        "\n",
        "        self.conv_layer2 = My2DConvLayer(kernel_size=(5, 5), rotation_speed=1, in_channels=16, out_channels=16)\n",
        "        self.celu2 = ComplexRelu()\n",
        "        self.bn2 = BatchNorm(16, 4)\n",
        "        self.avgpool2 = ComplexMeanPooling(kernel_size=(3,3), stride=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(576, 10)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(10, num_classes)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "    \n",
        "    # Progresses data across layers    \n",
        "    def forward(self, x):\n",
        "        out = self.conv_layer1(x)\n",
        "        out = self.celu1(out)\n",
        "        out = self.bn1(out)\n",
        "        out = self.avgpool1(out)\n",
        "        out = self.conv_layer2(out)\n",
        "        out = self.celu2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.avgpool2(out)\n",
        "        out = out.float()\n",
        "        out = out.reshape(out.size(0), -1).float()\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.softmax(out)\n",
        "        # print(out[0])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "e3a19e7f-01e2-4513-b7a2-d299ab503e05",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-28T22:25:56.287630Z",
          "iopub.status.busy": "2022-10-28T22:25:56.286705Z",
          "iopub.status.idle": "2022-10-28T22:25:56.301105Z",
          "shell.execute_reply": "2022-10-28T22:25:56.299771Z",
          "shell.execute_reply.started": "2022-10-28T22:25:56.287559Z"
        },
        "id": "e3a19e7f-01e2-4513-b7a2-d299ab503e05",
        "tags": []
      },
      "outputs": [],
      "source": [
        "modelH = HarmonicNeuralNet(num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(modelH.parameters())  \n",
        "num_epochs=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "c4557b71-33f8-4023-ac7b-10674e54707d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-10-28T22:25:56.837774Z",
          "iopub.status.busy": "2022-10-28T22:25:56.837378Z",
          "iopub.status.idle": "2022-10-28T22:25:56.938624Z",
          "shell.execute_reply": "2022-10-28T22:25:56.937489Z",
          "shell.execute_reply.started": "2022-10-28T22:25:56.837736Z"
        },
        "id": "c4557b71-33f8-4023-ac7b-10674e54707d",
        "outputId": "f6c87358-db48-404e-88ba-a398ced0a2f1",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "......................................................................................................\n",
            "Epoch [1/10], Loss: 1.4506\n",
            "......................................................................................................\n",
            "Epoch [2/10], Loss: 1.4072\n",
            "......................................................................................................\n",
            "Epoch [3/10], Loss: 1.1515\n",
            "......................................................................................................\n",
            "Epoch [4/10], Loss: 0.8748\n",
            "......................................................................................................\n",
            "Epoch [5/10], Loss: 1.1314\n",
            "......................................................................................................\n",
            "Epoch [6/10], Loss: 1.0970\n",
            "......................................................................................................\n",
            "Epoch [7/10], Loss: 0.6587\n",
            "......................................................................................................\n",
            "Epoch [8/10], Loss: 0.5328\n",
            "......................................................................................................\n",
            "Epoch [9/10], Loss: 0.4577\n",
            "......................................................................................................\n",
            "Epoch [10/10], Loss: 0.4575\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    #Load in the data in batches using the train_loader object\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # with torch.autograd.detect_anomaly():\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = modelH(images.to(device))\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "        print('.',end='')\n",
        "        if i>100: break\n",
        "    print('\\nEpoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "5Zkad3BLH9mU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zkad3BLH9mU",
        "outputId": "17948355-954e-4213-fb56-f64140f8ed47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "HarmonicNeuralNet(\n",
              "  (conv_layer1): My2DConvLayer()\n",
              "  (celu1): ComplexRelu()\n",
              "  (bn1): BatchNorm()\n",
              "  (avgpool1): ComplexMeanPooling()\n",
              "  (conv_layer2): My2DConvLayer()\n",
              "  (celu2): ComplexRelu()\n",
              "  (bn2): BatchNorm()\n",
              "  (avgpool2): ComplexMeanPooling()\n",
              "  (fc1): Linear(in_features=576, out_features=10, bias=True)\n",
              "  (relu1): ReLU()\n",
              "  (fc2): Linear(in_features=10, out_features=10, bias=True)\n",
              "  (relu2): ReLU()\n",
              "  (softmax): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Specify a path\n",
        "PATH = \"state_dict_model.pt\"\n",
        "\n",
        "# Save\n",
        "torch.save(modelH.state_dict(), PATH)\n",
        "\n",
        "# Load\n",
        "\n",
        "model = HarmonicNeuralNet(10).to(device)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
